{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, platform\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps USE_AMP: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/gpgs73x92nn02hjw6w7j3spm0000gn/T/ipykernel_49946/1418182277.py:5: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n"
     ]
    }
   ],
   "source": [
    "#select computation device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    USE_AMP = True\n",
    "elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    USE_AMP = False\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    USE_AMP = False\n",
    "\n",
    "print(\"Device:\", DEVICE, \"USE_AMP:\", USE_AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "IMG_SIZE = 512       \n",
    "BATCH_SIZE = 6\n",
    "NUM_EPOCHS = 12\n",
    "LR = 1e-4 #changed\n",
    "CONTENT_WEIGHT = 1 #\n",
    "STYLE_WEIGHT   = 1e6   \n",
    "TV_WEIGHT      = 1e-6 #for smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fielpaths\n",
    "CONTENT_ROOT = \"../Data/dataset/clean/animals_balanced\"   \n",
    "STYLE_ROOT= \"../Data/dataset/clean/origami_images\"\n",
    "SPLIT_ROOT   = \"../Data/dataset/split\"   \n",
    "CHECKPOINT_DIR = \"./checkpoints_nststyle\"\n",
    "SAMPLES_DIR    = \"./samples_nststyle\"\n",
    "\n",
    "for d in [SPLIT_ROOT, CHECKPOINT_DIR, SAMPLES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "TARGET_CLASS = \"butterfly\" #single class (inital)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for root in ['content', 'style']:\n",
    "        path = os.path.join(SPLIT_ROOT, root, split, TARGET_CLASS)\n",
    "        os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_WORKERS: 0 PIN_MEMORY: False\n"
     ]
    }
   ],
   "source": [
    "#uses dataloader for windows devices\n",
    "if platform.system().lower().startswith(\"darwin\"):\n",
    "    NUM_WORKERS = 0\n",
    "else:\n",
    "    NUM_WORKERS = min(6, max(0, (os.cpu_count() or 2) - 2))\n",
    "\n",
    "PIN_MEMORY = True if DEVICE.type == \"cuda\" else False\n",
    "print(\"NUM_WORKERS:\", NUM_WORKERS, \"PIN_MEMORY:\", PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG Layer Configs and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vsame as NST\n",
    "LAYER_INDICES = {\n",
    "    'conv1_1': '0', \n",
    "    'conv1_2': '2', \n",
    "    'conv2_1': '5', \n",
    "    'conv2_2': '7',\n",
    "    'conv3_1': '10', \n",
    "    'conv3_2': '12', \n",
    "    'conv3_3': '14', \n",
    "    'conv3_4': '16',\n",
    "    'conv4_1': '19', \n",
    "    'conv4_2': '21', \n",
    "    'conv4_3': '23', \n",
    "    'conv4_4': '25',\n",
    "    'conv5_1': '28', \n",
    "    'conv5_2': '30', \n",
    "    'conv5_3': '32', \n",
    "    'conv5_4': '34'\n",
    "}\n",
    "\n",
    "LAYER_CONFIGS = {\n",
    "    'gatys': {\n",
    "        'content': ['conv4_2'],\n",
    "        'style': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1'],\n",
    "        'style_weights': {\n",
    "            'conv1_1': 1.0,\n",
    "            'conv2_1': 0.8,\n",
    "            'conv3_1': 0.5,\n",
    "            'conv4_1': 0.3,\n",
    "            'conv5_1': 0.1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "ACTIVE_LAYER_CONFIG = 'gatys'\n",
    "\n",
    "#normalization\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization for VGG and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exif_fix_and_open(path):\n",
    "    img = Image.open(path)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "#same as nst.py\n",
    "def normalize_for_vgg(x):\n",
    "    mean = torch.tensor(IMG_MEAN).view(1,3,1,1).to(DEVICE)\n",
    "    std  = torch.tensor(IMG_STD).view(1,3,1,1).to(DEVICE)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# def extract_features_batch(x, layers, model):\n",
    "#     x_vgg = normalize_for_vgg(x)\n",
    "#     cur = x_vgg\n",
    "#     features = {}\n",
    "#     layers_to_extract = {LAYER_INDICES[name]: name for name in layers}\n",
    "#     for idx, layer in model._modules.items():\n",
    "#         cur = layer(cur)\n",
    "#         if idx in layers_to_extract:\n",
    "#             features[layers_to_extract[idx]] = cur\n",
    "#     return features\n",
    "\n",
    "def gram_matrix_batch(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    f = tensor.view(b, c, h*w)\n",
    "    return torch.bmm(f, f.transpose(1,2)) / (c * h * w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content - butterfly - found 1160 files in ../Data/dataset/clean/animals_balanced/butterfly\n",
      "style - butterfly - found 116 files in ../Data/dataset/clean/origami_images/butterfly\n",
      "Created train/val/test splits under ../Data/dataset/split/butterfly\n"
     ]
    }
   ],
   "source": [
    "def ensure_splits(class_name, val_frac=0.1, test_frac=0.05, seed=42):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    src_content = os.path.join(CONTENT_ROOT, class_name)\n",
    "    src_style   = os.path.join(STYLE_ROOT, class_name)\n",
    "    \n",
    "    assert os.path.isdir(src_content), f\"Missing content folder: {src_content}\"\n",
    "    assert os.path.isdir(src_style), f\"Missing style folder: {src_style}\"\n",
    "\n",
    "    def split_list(files):\n",
    "        n = len(files)\n",
    "        \n",
    "        n_val = int(n * val_frac)\n",
    "        n_test = int(n * test_frac)\n",
    "        \n",
    "        return files[n_val+n_test:], files[:n_val], files[n_val:n_val+n_test]\n",
    "\n",
    "    def copy_split(src_folder, dst_folder, files):#copy to split folder\n",
    "        os.makedirs(dst_folder, exist_ok=True)\n",
    "        \n",
    "        for f in files:\n",
    "            src = os.path.join(src_folder, f)\n",
    "            dst = os.path.join(dst_folder, f)\n",
    "            \n",
    "            if not os.path.exists(dst):\n",
    "                Image.open(src).convert(\"RGB\").save(dst, \"JPEG\", quality=90)\n",
    "\n",
    "    for domain in [\"content\", \"style\"]: #for both sets of images\n",
    "        src = os.path.join(CONTENT_ROOT if domain==\"content\" else STYLE_ROOT, class_name)\n",
    "        files = [f for f in os.listdir(src) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        \n",
    "        print(f\"{domain} - {class_name} - found {len(files)} files in {src}\")\n",
    "        \n",
    "        random.shuffle(files)\n",
    "        train, val, test = split_list(files)\n",
    "\n",
    "        for split_name, flist in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
    "            out_dir = os.path.join(SPLIT_ROOT, domain, split_name, class_name)\n",
    "            copy_split(src, out_dir, flist)\n",
    "\n",
    "    print(f\"Created train/val/test splits under {SPLIT_ROOT}/{class_name}\")\n",
    "\n",
    "ensure_splits(TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progressive training for GPU meomory\n",
    "\n",
    "IMG_SIZE_STAGE1 = 256   \n",
    "IMG_SIZE_STAGE2 = 512   \n",
    "NUM_EPOCHS_STAGE1 = 8\n",
    "NUM_EPOCHS_STAGE2 = 3\n",
    "\n",
    "def get_transforms(img_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "content_transform_stage1 = get_transforms(IMG_SIZE_STAGE1)\n",
    "style_transform_stage1   = content_transform_stage1\n",
    "\n",
    "content_transform_stage2 = get_transforms(IMG_SIZE_STAGE2)\n",
    "style_transform_stage2   = content_transform_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler ready, train content files: 986\n",
      "Sampler ready, train style files: 100\n"
     ]
    }
   ],
   "source": [
    "class SingleClassPairedSampler:\n",
    "    \n",
    "    def __init__(self, split_root, class_name, transform=None, use_dataloader=(NUM_WORKERS>0)):\n",
    "        self.split_root = split_root\n",
    "        self.class_name = class_name\n",
    "        self._build_index()\n",
    "        self.split = 'train'\n",
    "        self.transform = transform\n",
    "        self._use_dataloader = use_dataloader\n",
    "        self._init_dataloaders_if_needed()\n",
    "\n",
    "    #build index\n",
    "    def _build_index(self):\n",
    "        for split in ['train','val','test']:\n",
    "            c_path = os.path.join(self.split_root, \"content\", split, self.class_name)\n",
    "            s_path = os.path.join(self.split_root, \"style\",   split, self.class_name)\n",
    "\n",
    "            c_files = sorted([\n",
    "                os.path.join(c_path, f) for f in os.listdir(c_path)\n",
    "                if f.lower().endswith(('.jpg','.jpeg','.png'))\n",
    "            ])\n",
    "            s_files = sorted([\n",
    "                os.path.join(s_path, f) for f in os.listdir(s_path)\n",
    "                if f.lower().endswith(('.jpg','.jpeg','.png'))\n",
    "            ])\n",
    "\n",
    "            setattr(self, f\"{split}_content_files\", c_files)\n",
    "            setattr(self, f\"{split}_style_files\", s_files)\n",
    "        \n",
    "\n",
    "    def _init_dataloaders_if_needed(self): #only for non mac devices\n",
    "        if not self._use_dataloader:\n",
    "            self._content_loader = None\n",
    "            self._style_loader = None\n",
    "            return\n",
    "        \n",
    "        class _DS(Dataset):\n",
    "            def __init__(self, files, transform):\n",
    "                self.files = files\n",
    "                self.transform = transform\n",
    "                \n",
    "            def __len__(self): return len(self.files)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                img = exif_fix_and_open(self.files[idx])\n",
    "                return self.transform(img)\n",
    "            \n",
    "        c_ds = _DS(getattr(self, \"train_content_files\"), self.transform or content_transform_stage1)\n",
    "        s_ds = _DS(getattr(self, \"train_style_files\"), self.transform or style_transform_stage1)\n",
    "        \n",
    "        self._content_loader = DataLoader(c_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        self._style_loader   = DataLoader(s_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        \n",
    "        self._content_iter = iter(self._content_loader)\n",
    "        self._style_iter = iter(self._style_loader)\n",
    "\n",
    "    def set_split(self, split):#switches b/w train/test/val\n",
    "        self.split = split\n",
    "        # if not using dataloader, nothing else to do\n",
    "        if not self._use_dataloader:\n",
    "            return\n",
    "        # rebuild dataloaders for chosen split\n",
    "        class _DS(Dataset):\n",
    "            def __init__(self, files, transform):\n",
    "                self.files = files\n",
    "                self.transform = transform\n",
    "                \n",
    "            def __len__(self): return len(self.files)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                img = exif_fix_and_open(self.files[idx])\n",
    "                return self.transform(img)\n",
    "            \n",
    "        c_files = getattr(self, f\"{split}_content_files\")\n",
    "        s_files = getattr(self, f\"{split}_style_files\")\n",
    "        \n",
    "        c_ds = _DS(c_files, self.transform or content_transform_stage1)\n",
    "        s_ds = _DS(s_files, self.transform or style_transform_stage1)\n",
    "        \n",
    "        self._content_loader = DataLoader(c_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        self._style_loader   = DataLoader(s_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "        \n",
    "        self._content_iter = iter(self._content_loader)\n",
    "        self._style_iter = iter(self._style_loader)\n",
    "\n",
    "    #returns a batach of paired content\n",
    "    def sample_batch(self, batch_size):\n",
    "        if self._use_dataloader:\n",
    "            # dataloader path (works)\n",
    "            try:\n",
    "                c = next(self._content_iter)\n",
    "            except Exception:\n",
    "                self._content_iter = iter(self._content_loader)\n",
    "                c = next(self._content_iter)\n",
    "            try:\n",
    "                s = next(self._style_iter)\n",
    "            except Exception:\n",
    "                self._style_iter = iter(self._style_loader)\n",
    "                s = next(self._style_iter)\n",
    "            return c, s\n",
    "        else:\n",
    "            # fallback if not using dataloader\n",
    "            c_files = getattr(self, f\"{self.split}_content_files\")\n",
    "            s_files = getattr(self, f\"{self.split}_style_files\")\n",
    "\n",
    "            paths_c = random.choices(c_files, k=batch_size)\n",
    "            paths_s = random.choices(s_files, k=batch_size)\n",
    "\n",
    "            c_imgs = [content_transform_stage1(exif_fix_and_open(p)) for p in paths_c]\n",
    "            s_imgs = [style_transform_stage1(exif_fix_and_open(p)) for p in paths_s]\n",
    "\n",
    "            return torch.stack(c_imgs), torch.stack(s_imgs)\n",
    "        \n",
    "sampler = SingleClassPairedSampler(SPLIT_ROOT, TARGET_CLASS, transform=content_transform_stage1)\n",
    "\n",
    "print(\"Sampler ready, train content files:\", len(sampler.train_content_files))\n",
    "print(\"Sampler ready, train style files:\", len(sampler.train_style_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pre-Trained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG feature extractor ready.\n"
     ]
    }
   ],
   "source": [
    "#load pretrained VGG (same form nst.py)\n",
    "vgg = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vgg, layer_indices):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg\n",
    "        self.idx_to_name = {int(idx_str): name for name, idx_str in layer_indices.items()}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cur = x\n",
    "        feats = {}\n",
    "        \n",
    "        for idx, layer in self.vgg._modules.items():\n",
    "            cur = layer(cur)\n",
    "            i = int(idx)\n",
    "            if i in self.idx_to_name:\n",
    "                feats[self.idx_to_name[i]] = cur\n",
    "        return feats\n",
    "\n",
    "vgg_feat = VGGFeatureExtractor(vgg, LAYER_INDICES).to(DEVICE).eval()\n",
    "\n",
    "print(\"VGG feature extractor ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module): #processes the image and extracts features while keeping style consistent\n",
    "    def __init__(self, in_c, out_c, kernel, stride):\n",
    "        super().__init__()\n",
    "        padding = kernel // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel, stride, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.inorm(self.conv(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module): #learn style modifications\n",
    "#     def __init__(self, channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.in1(self.conv1(x)))\n",
    "#         out = self.in2(self.conv2(out))\n",
    "#         return out + x\n",
    "\n",
    "class StylizedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        \n",
    "        # style gate to enhance stylized contrast/edges\n",
    "        self.style_gate = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        gate = self.style_gate(out)\n",
    "        # modulate residual with learned style gate\n",
    "        out = out * gate + x\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleConv(nn.Module): # upsampling the image (making it bigger)\n",
    "    def __init__(self, in_c, out_c, kernel, upsample=None):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        padding = kernel // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel, 1, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=self.upsample, mode='nearest')\n",
    "            \n",
    "        return F.relu(self.inorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerNet(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "#         self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "#         self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "#         self.res1 = ResidualBlock(128)\n",
    "#         self.res2 = ResidualBlock(128)\n",
    "#         self.res3 = ResidualBlock(128)\n",
    "#         self.res4 = ResidualBlock(128)\n",
    "#         self.res5 = ResidualBlock(128)\n",
    "#         self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "#         self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "#         self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.conv3(y)\n",
    "#         y = self.res1(y)\n",
    "#         y = self.res2(y)\n",
    "#         y = self.res3(y)\n",
    "#         y = self.res4(y)\n",
    "#         y = self.res5(y)\n",
    "#         y = self.up1(y)\n",
    "#         y = self.up2(y)\n",
    "#         y = self.conv_out(y)\n",
    "#         return torch.sigmoid(y)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "        self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "        self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "\n",
    "        #stylized Residuals\n",
    "        self.res1 = StylizedResidualBlock(128)\n",
    "        self.res2 = StylizedResidualBlock(128)\n",
    "        self.res3 = StylizedResidualBlock(128)\n",
    "        self.res4 = StylizedResidualBlock(128)\n",
    "        self.res5 = StylizedResidualBlock(128)\n",
    "\n",
    "        self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "        self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "        self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.up1(y)\n",
    "        y = self.up2(y)\n",
    "        y = self.conv_out(y)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed 116 style gram dicts.\n"
     ]
    }
   ],
   "source": [
    "def precompute_style_grams(style_dir, transform, batch_size=8):\n",
    "    files = [os.path.join(style_dir, f) for f in os.listdir(style_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "    \n",
    "    class _DS(Dataset):\n",
    "        def __init__(self, files, transform):\n",
    "            self.files = files\n",
    "            self.transform = transform\n",
    "            \n",
    "        def __len__(self): return len(self.files)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.transform(exif_fix_and_open(self.files[idx]))\n",
    "        \n",
    "    ds = _DS(files, transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=max(0, NUM_WORKERS//2))\n",
    "    grams_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            feats = vgg_feat(normalize_for_vgg(batch))\n",
    "            \n",
    "            for i in range(batch.size(0)):\n",
    "                gdict = {l: gram_matrix_batch(feats[l][i:i+1]).cpu() for l in feats.keys()}\n",
    "                grams_list.append(gdict)\n",
    "                \n",
    "    print(f\"Precomputed {len(grams_list)} style gram dicts.\")\n",
    "    return grams_list\n",
    "\n",
    "style_dir = os.path.join(STYLE_ROOT, TARGET_CLASS)\n",
    "\n",
    "style_grams_stage1 = precompute_style_grams(style_dir, style_transform_stage1, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model and optimizer\n",
    "model = TransformerNet().to(DEVICE)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss_fn(x):\n",
    "    return torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:])) + \\\n",
    "           torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
    "\n",
    "#train\n",
    "def train_stage(model, sampler, style_grams, transform, num_epochs, stage_name):\n",
    "    sampler.transform = transform\n",
    "    sampler._init_dataloaders_if_needed()\n",
    "    sampler.set_split('train')\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler() if (USE_AMP and DEVICE.type == \"cuda\") else None #for gpu\n",
    "\n",
    "    #finds number of steps\n",
    "    n_train = len(getattr(sampler, \"train_content_files\", []))\n",
    "    steps_per_epoch = max(100, n_train // BATCH_SIZE)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(range(steps_per_epoch), desc=f\"{stage_name} Epoch {epoch}\") #progress bar\n",
    "\n",
    "        for step in pbar: \n",
    "            content_batch, style_batch = sampler.sample_batch(BATCH_SIZE)\n",
    "            content_batch = content_batch.to(DEVICE)\n",
    "            style_batch = style_batch.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            if scaler is not None: #windows\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(content_batch)\n",
    "                    c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(output)\n",
    "                    c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm)\n",
    "\n",
    "                    # Content loss\n",
    "                    c_loss = sum(torch.mean((o_feats[l] - c_feats[l])**2) for l in content_layers)\n",
    "\n",
    "                    # Style loss\n",
    "                    Gs = random.choice(style_grams)\n",
    "                    s_loss = 0.0\n",
    "                    for l in style_layers:\n",
    "                        Go = gram_matrix_batch(o_feats[l])\n",
    "                        s_loss += style_weights.get(l, 1.0) * torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "\n",
    "                    # TV loss\n",
    "                    tv = tv_loss_fn(output)\n",
    "                    total_loss = CONTENT_WEIGHT * c_loss + STYLE_WEIGHT * s_loss + TV_WEIGHT * tv\n",
    "\n",
    "                # Backprop with scaler\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "\n",
    "                output = model(content_batch) #output image\n",
    "                \n",
    "                c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(output) #normalize\n",
    "                c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm) #extract features\n",
    "\n",
    "                c_loss = sum(torch.mean((o_feats[l] - c_feats[l])**2) for l in content_layers) #content loss\n",
    "\n",
    "                #style loss\n",
    "                Gs = random.choice(style_grams)\n",
    "                s_loss = 0.0\n",
    "                for l in style_layers:\n",
    "                    Go = gram_matrix_batch(o_feats[l])\n",
    "                    s_loss += style_weights.get(l, 1.0) * torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "\n",
    "                tv = tv_loss_fn(output)#TV loss\n",
    "                total_loss = CONTENT_WEIGHT * c_loss + STYLE_WEIGHT * s_loss + TV_WEIGHT * tv\n",
    "\n",
    "                total_loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "        \n",
    "            if step % 50 == 0:\n",
    "                pbar.set_description(f\"{stage_name} E{epoch} S{step} Loss {total_loss.item():.4f}\")\n",
    "\n",
    "            #save samples\n",
    "            if step % 300 == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    sample_out = model(content_batch[:1]).cpu()\n",
    "                    utils.save_image(sample_out, f\"{SAMPLES_DIR}/{stage_name}_ep{epoch}_step{step}.png\")\n",
    "                    \n",
    "                model.train()\n",
    "\n",
    "        # Save checkpoint and print epoch stats\n",
    "        avg_loss = epoch_loss / steps_per_epoch\n",
    "        \n",
    "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{stage_name}_epoch{epoch}.pth\"))\n",
    "        \n",
    "        print(f\"[{stage_name}] Epoch {epoch} done | Avg loss {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 1 (low-res) training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240cd94f9ba047069852c1fe3859fc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 1:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 1 done | Avg loss 109.1278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca657a994ada42aaad6dec27f4c67a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 2:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 2 done | Avg loss 82.5825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7320fefc21c34671bb5940cfe822f5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 3:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 3 done | Avg loss 85.0462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe927a3f64ab4ee2bf6461c02607dcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 4:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 4 done | Avg loss 98.4891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fe7b56ecb04fb8b1f9906be8266b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 5:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 5 done | Avg loss 86.0847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849b812e378b4415826607f030c12db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 6:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 6 done | Avg loss 91.3243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31df3e8d41214404a4e649f0aecdf930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 7:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 7 done | Avg loss 83.1822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d434104db0cd44d7b8988533b63a5f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage1_256 Epoch 8:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage1_256] Epoch 8 done | Avg loss 83.7357\n",
      "Precomputing style grams for stage2 (high-res)...\n",
      "Precomputed 116 style gram dicts.\n",
      "Starting Stage 2 (high-res) fine-tune\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c0a6329f61463d9255fab85ddf99ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage2_512 Epoch 1:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage2_512] Epoch 1 done | Avg loss 58.6360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a1e77af39740049bca02e98f8271ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage2_512 Epoch 2:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage2_512] Epoch 2 done | Avg loss 60.5667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee12a5d52aa4901b871171d63b36043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stage2_512 Epoch 3:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage2_512] Epoch 3 done | Avg loss 46.4449\n"
     ]
    }
   ],
   "source": [
    "#Stage 1: low res\n",
    "sampler = SingleClassPairedSampler(SPLIT_ROOT, TARGET_CLASS, transform=content_transform_stage1, use_dataloader=(NUM_WORKERS>0))\n",
    "style_grams = style_grams_stage1  #precomputed earlier\n",
    "\n",
    "print(\"Starting Stage 1 (low-res) training\")\n",
    "train_stage(model, sampler, style_grams, content_transform_stage1, NUM_EPOCHS_STAGE1, stage_name=f\"stage1_{IMG_SIZE_STAGE1}\")\n",
    "\n",
    "#Stage 2: high res\n",
    "print(\"Precomputing style grams for stage2 (high-res)...\")\n",
    "style_grams_stage2 = precompute_style_grams(style_dir, style_transform_stage2, batch_size=4)\n",
    "\n",
    "sampler = SingleClassPairedSampler(SPLIT_ROOT, TARGET_CLASS, transform=content_transform_stage2, use_dataloader=(NUM_WORKERS>0))\n",
    "print(\"Starting Stage 2 (high-res) fine-tune\")\n",
    "train_stage(model, sampler, style_grams_stage2, content_transform_stage2, NUM_EPOCHS_STAGE2, stage_name=f\"stage2_{IMG_SIZE_STAGE2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded stylized residual checkpoint successfully!\n"
     ]
    }
   ],
   "source": [
    "model = TransformerNet().to(DEVICE)\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/stage2_512_epoch3.pth\"  # use the stylized residual version\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Loaded stylized residual checkpoint successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: ./checkpoints_nststyle/stage2_512_epoch3.pth\n",
      "Saved stylized image to ./samples_nststyle/stylized_image.png\n"
     ]
    }
   ],
   "source": [
    "ckpt = os.path.join(CHECKPOINT_DIR, f\"stage2_{IMG_SIZE_STAGE2}_epoch{NUM_EPOCHS_STAGE2}.pth\")\n",
    "\n",
    "if os.path.exists(ckpt):\n",
    "    print(\"Loading checkpoint:\", ckpt)\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=DEVICE))\n",
    "    \n",
    "else:\n",
    "    print(\"Checkpoint not found; using model in memory (may be already trained).\")\n",
    "\n",
    "model.eval()\n",
    "test_img_path = \"test_imgs/butterfly.jpg\"\n",
    "\n",
    "if not os.path.exists(test_img_path):\n",
    "    print(\"Test image not found at\", test_img_path)\n",
    "else:\n",
    "    img = exif_fix_and_open(test_img_path)\n",
    "    \n",
    "    tf = get_transforms = transforms.Compose([transforms.Resize((IMG_SIZE_STAGE2, IMG_SIZE_STAGE2)),ctransforms.ToTensor()])\n",
    "    content_tensor = tf(img).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(content_tensor)\n",
    "        \n",
    "    utils.save_image(out.cpu(), os.path.join(SAMPLES_DIR, \"stylized_image.png\"))\n",
    "    \n",
    "    print(\"Saved stylized image to\", os.path.join(SAMPLES_DIR, \"stylized_image.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
