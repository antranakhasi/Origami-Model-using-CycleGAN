{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps USE_AMP: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/gpgs73x92nn02hjw6w7j3spm0000gn/T/ipykernel_84909/3228998659.py:5: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n"
     ]
    }
   ],
   "source": [
    "#select computation device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    USE_AMP = True\n",
    "elif getattr(torch, \"has_mps\", False) and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    USE_AMP = False\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    USE_AMP = False\n",
    "\n",
    "print(\"Device:\", DEVICE, \"USE_AMP:\", USE_AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 6\n",
    "NUM_EPOCHS = 12\n",
    "LR = 1e-4\n",
    "CONTENT_WEIGHT = 1.0\n",
    "STYLE_WEIGHT = 1e6\n",
    "TV_WEIGHT = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fielpaths\n",
    "CONTENT_ROOT = \"../Data/dataset/clean/animals_balanced\"   \n",
    "STYLE_ROOT= \"../Data/dataset/clean/origami_images\"\n",
    "SPLIT_ROOT   = \"../Data/dataset/split\"   \n",
    "CHECKPOINT_DIR = \"./checkpoints_nststyle\"\n",
    "SAMPLES_DIR    = \"./samples_nststyle\"\n",
    "\n",
    "for d in [SPLIT_ROOT, CHECKPOINT_DIR, SAMPLES_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TARGET_CLASS = \"butterfly\" #single class (inital)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for root in ['content', 'style']:\n",
    "        path = os.path.join(SPLIT_ROOT, root, split, TARGET_CLASS)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vsame as NST\n",
    "LAYER_INDICES = {\n",
    "    'conv1_1': '0', \n",
    "    'conv1_2': '2', \n",
    "    'conv2_1': '5', \n",
    "    'conv2_2': '7',\n",
    "    'conv3_1': '10', \n",
    "    'conv3_2': '12', \n",
    "    'conv3_3': '14', \n",
    "    'conv3_4': '16',\n",
    "    'conv4_1': '19', \n",
    "    'conv4_2': '21', \n",
    "    'conv4_3': '23', \n",
    "    'conv4_4': '25',\n",
    "    'conv5_1': '28', \n",
    "    'conv5_2': '30', \n",
    "    'conv5_3': '32', \n",
    "    'conv5_4': '34'\n",
    "}\n",
    "\n",
    "LAYER_CONFIGS = {\n",
    "    'gatys': {\n",
    "        'content': ['conv4_2'],\n",
    "        'style': ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1'],\n",
    "        'style_weights': {\n",
    "            'conv1_1': 1.0,\n",
    "            'conv2_1': 0.8,\n",
    "            'conv3_1': 0.5,\n",
    "            'conv4_1': 0.3,\n",
    "            'conv5_1': 0.1\n",
    "        },\n",
    "    }\n",
    "}\n",
    "ACTIVE_LAYER_CONFIG = 'gatys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def list_images(dir_):\n",
    "    return sorted([os.path.join(dir_,f) for f in os.listdir(dir_)\n",
    "                   if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "    \n",
    "def exif_fix_and_open(path):\n",
    "    img = Image.open(path)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    return img.convert(\"RGB\")\n",
    "\n",
    "def print_progress(prefix, step, total, every=10, end=False):\n",
    "    if end:\n",
    "        print(f\"\\n{prefix} done.\")\n",
    "        return\n",
    "    if step % every == 0:\n",
    "        print(\"==> \" + \".\" * (step // every), end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as nst.py\n",
    "def normalize_for_vgg(x):\n",
    "    mean = torch.tensor(IMG_MEAN, device=DEVICE).view(1,3,1,1)\n",
    "    std  = torch.tensor(IMG_STD,  device=DEVICE).view(1,3,1,1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def gram_matrix_batch(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    f = tensor.view(b, c, h*w)\n",
    "    return torch.bmm(f, f.transpose(1,2)) / (c * h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits ready under ../Data/dataset/split/butterfly\n"
     ]
    }
   ],
   "source": [
    "def ensure_splits(class_name, val_frac=0.1, test_frac=0.05, seed=42):\n",
    "    random.seed(seed)\n",
    "    src_content = os.path.join(CONTENT_ROOT, class_name)\n",
    "    src_style   = os.path.join(STYLE_ROOT,   class_name)\n",
    "    assert os.path.isdir(src_content), f\"Missing content folder: {src_content}\"\n",
    "    assert os.path.isdir(src_style),   f\"Missing style folder: {src_style}\"\n",
    "\n",
    "    def split_list(files):\n",
    "        n = len(files)\n",
    "        n_val  = int(n * val_frac)\n",
    "        n_test = int(n * test_frac)\n",
    "        return files[n_val+n_test:], files[:n_val], files[n_val:n_val+n_test]\n",
    "\n",
    "    def copy_split(src_folder, dst_folder, files):\n",
    "        os.makedirs(dst_folder, exist_ok=True)\n",
    "        for f in files:\n",
    "            src = os.path.join(src_folder, f)\n",
    "            dst = os.path.join(dst_folder, f)\n",
    "            if not os.path.exists(dst):\n",
    "                exif_fix_and_open(src).save(dst, \"JPEG\", quality=90)\n",
    "\n",
    "    for domain, root in [(\"content\", CONTENT_ROOT), (\"style\", STYLE_ROOT)]:\n",
    "        src = os.path.join(root, class_name)\n",
    "        files = [f for f in os.listdir(src) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "        print(f\"{domain} - {class_name} - found {len(files)} files in {src}\")\n",
    "        random.shuffle(files)\n",
    "        train, val, test = split_list(files)\n",
    "        for split_name, flist in zip([\"train\",\"val\",\"test\"], [train, val, test]):\n",
    "            out_dir = os.path.join(SPLIT_ROOT, domain, split_name, class_name)\n",
    "            copy_split(src, out_dir, flist)\n",
    "\n",
    "    print(f\"Created train/val/test splits under {SPLIT_ROOT}/{class_name}\")\n",
    "\n",
    "ensure_splits(TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_img = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleClassPairedSampler:\n",
    "    def __init__(self, split_root, class_name, transform):\n",
    "        self.split_root = split_root\n",
    "        self.class_name = class_name\n",
    "        self.transform  = transform\n",
    "        self.split = 'train'\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for split in ['train','val','test']:\n",
    "            c_path = os.path.join(self.split_root, \"content\", split, self.class_name)\n",
    "            s_path = os.path.join(self.split_root, \"style\",   split, self.class_name)\n",
    "            c_files = sorted([os.path.join(c_path,f) for f in os.listdir(c_path)\n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "            s_files = sorted([os.path.join(s_path,f) for f in os.listdir(s_path)\n",
    "                              if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "            setattr(self, f\"{split}_content_files\", c_files)\n",
    "            setattr(self, f\"{split}_style_files\",   s_files)\n",
    "\n",
    "    def set_split(self, split):\n",
    "        self.split = split\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        c_files = getattr(self, f\"{self.split}_content_files\")\n",
    "        s_files = getattr(self, f\"{self.split}_style_files\")\n",
    "        paths_c = random.choices(c_files, k=batch_size)\n",
    "        paths_s = random.choices(s_files, k=batch_size)\n",
    "        c_imgs = [self.transform(exif_fix_and_open(p)) for p in paths_c]\n",
    "        s_imgs = [self.transform(exif_fix_and_open(p)) for p in paths_s]\n",
    "        return torch.stack(c_imgs), torch.stack(s_imgs)\n",
    "\n",
    "sampler = SingleClassPairedSampler(SPLIT_ROOT, TARGET_CLASS, transform=transform_img)\n",
    "print(\"Sampler ready | train content:\", len(sampler.train_content_files), \"| train style:\", len(sampler.train_style_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, vgg, layer_indices):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg\n",
    "        self.idx_to_name = {int(idx_str): name for name, idx_str in layer_indices.items()}\n",
    "    def forward(self, x):\n",
    "        cur = x\n",
    "        feats = {}\n",
    "        for idx, layer in self.vgg._modules.items():\n",
    "            cur = layer(cur)\n",
    "            i = int(idx)\n",
    "            if i in self.idx_to_name:\n",
    "                feats[self.idx_to_name[i]] = cur\n",
    "        return feats\n",
    "\n",
    "vgg_feat = VGGFeatureExtractor(vgg, LAYER_INDICES).to(DEVICE).eval()\n",
    "\n",
    "# Active layer config\n",
    "_cfg = LAYER_CONFIGS[ACTIVE_LAYER_CONFIG]\n",
    "content_layers  = _cfg['content']\n",
    "style_layers    = _cfg['style']\n",
    "style_weights   = _cfg['style_weights']\n",
    "print(\"Content layers:\", content_layers)\n",
    "print(\"Style layers:\", style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel, stride):\n",
    "        super().__init__()\n",
    "        padding = kernel // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel, stride, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.inorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module): #learn style modifications\n",
    "#     def __init__(self, channels):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#         self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "#         self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.in1(self.conv1(x)))\n",
    "#         out = self.in2(self.conv2(out))\n",
    "#         return out + x\n",
    "\n",
    "class StylizedResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in1   = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.in2   = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.style_gate = nn.Sequential(nn.Conv2d(channels, channels, 1), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        out  = F.relu(self.in1(self.conv1(x)))\n",
    "        out  = self.in2(self.conv2(out))\n",
    "        gate = self.style_gate(out)\n",
    "        return out * gate + x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleConv(nn.Module):\n",
    "    def __init__(self, in_c, out_c, kernel, upsample=None):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        padding = kernel // 2\n",
    "        self.conv  = nn.Conv2d(in_c, out_c, kernel, 1, padding)\n",
    "        self.inorm = nn.InstanceNorm2d(out_c, affine=True)\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=self.upsample, mode='nearest')\n",
    "        return F.relu(self.inorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerNet(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "#         self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "#         self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "#         self.res1 = ResidualBlock(128)\n",
    "#         self.res2 = ResidualBlock(128)\n",
    "#         self.res3 = ResidualBlock(128)\n",
    "#         self.res4 = ResidualBlock(128)\n",
    "#         self.res5 = ResidualBlock(128)\n",
    "#         self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "#         self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "#         self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.conv3(y)\n",
    "#         y = self.res1(y)\n",
    "#         y = self.res2(y)\n",
    "#         y = self.res3(y)\n",
    "#         y = self.res4(y)\n",
    "#         y = self.res5(y)\n",
    "#         y = self.up1(y)\n",
    "#         y = self.up2(y)\n",
    "#         y = self.conv_out(y)\n",
    "#         return torch.sigmoid(y)\n",
    "\n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvLayer(3, 32, 9, 1)\n",
    "        self.conv2 = ConvLayer(32, 64, 3, 2)\n",
    "        self.conv3 = ConvLayer(64, 128, 3, 2)\n",
    "        self.res1 = StylizedResidualBlock(128)\n",
    "        self.res2 = StylizedResidualBlock(128)\n",
    "        self.res3 = StylizedResidualBlock(128)\n",
    "        self.res4 = StylizedResidualBlock(128)\n",
    "        self.res5 = StylizedResidualBlock(128)\n",
    "        self.up1 = UpsampleConv(128, 64, 3, upsample=2)\n",
    "        self.up2 = UpsampleConv(64, 32, 3, upsample=2)\n",
    "        self.conv_out = nn.Conv2d(32, 3, 9, 1, 4)\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x); y = self.conv2(y); y = self.conv3(y)\n",
    "        y = self.res1(y);  y = self.res2(y);  y = self.res3(y); y = self.res4(y); y = self.res5(y)\n",
    "        y = self.up1(y);   y = self.up2(y)\n",
    "        y = self.conv_out(y)\n",
    "        return torch.sigmoid(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerNet().to(DEVICE)\n",
    "opt   = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def tv_loss_fn(x):\n",
    "    return torch.mean(torch.abs(x[:,:,:, :-1] - x[:,:,:,1:])) + \\\n",
    "           torch.mean(torch.abs(x[:,:, :-1,:] - x[:,:,1:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Gram Precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_style_grams(style_dir, transform):\n",
    "    files = [os.path.join(style_dir, f) for f in os.listdir(style_dir)\n",
    "             if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "    grams_list = []\n",
    "    with torch.no_grad():\n",
    "        for p in files:\n",
    "            x = transform(exif_fix_and_open(p)).unsqueeze(0).to(DEVICE)\n",
    "            feats = vgg_feat(normalize_for_vgg(x))\n",
    "            gdict = {l: gram_matrix_batch(feats[l]).cpu() for l in feats.keys()}\n",
    "            grams_list.append(gdict)\n",
    "    print(f\"Precomputed {len(grams_list)} style gram dicts.\")\n",
    "    return grams_list\n",
    "\n",
    "style_dir = os.path.join(STYLE_ROOT, TARGET_CLASS)\n",
    "style_grams = precompute_style_grams(style_dir, transform_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (no dataloader)â€¦\n",
      "Epoch 1: ====................  20.0% | loss: 24.14405E1 S200/1000 | loss 26.4994\n",
      "Epoch 1: ========............  40.0% | loss: 18.2296E1 S400/1000 | loss 21.0941\n",
      "Epoch 1: ============........  60.0% | loss: 14.4558E1 S600/1000 | loss 19.6946\n",
      "Epoch 1: ================....  80.0% | loss: 13.2063E1 S800/1000 | loss 17.7973\n",
      "Epoch 1: ==================== 100.0% | loss: 11.0591\n",
      "E1 S1000/1000 | loss 15.2456\n",
      "[E1] checkpoint saved.\n",
      "Epoch 2: ====................  20.0% | loss: 18.7341E2 S200/1000 | loss 14.3131\n",
      "Epoch 2: ========............  40.0% | loss: 8.60605E2 S400/1000 | loss 13.7263\n",
      "Epoch 2: ============........  60.0% | loss: 6.91944E2 S600/1000 | loss 13.0267\n",
      "Epoch 2: ================....  80.0% | loss: 7.74632E2 S800/1000 | loss 12.0806\n",
      "Epoch 2: ==================== 100.0% | loss: 7.44248\n",
      "E2 S1000/1000 | loss 11.1061\n",
      "[E2] checkpoint saved.\n",
      "Epoch 3: ====................  20.0% | loss: 10.1141E3 S200/1000 | loss 11.5658\n",
      "Epoch 3: ========............  40.0% | loss: 5.91340E3 S400/1000 | loss 10.8631\n",
      "Epoch 3: ============........  60.0% | loss: 9.52651E3 S600/1000 | loss 11.7877\n",
      "Epoch 3: ================....  80.0% | loss: 27.6707E3 S800/1000 | loss 12.1525\n",
      "Epoch 3: ==================== 100.0% | loss: 25.8874\n",
      "E3 S1000/1000 | loss 10.4699\n",
      "[E3] checkpoint saved.\n",
      "Epoch 4: ====................  20.0% | loss: 5.40036E4 S200/1000 | loss 11.5797\n",
      "Epoch 4: ========............  40.0% | loss: 24.5843E4 S400/1000 | loss 10.7392\n",
      "Epoch 4: ============........  60.0% | loss: 8.82764E4 S600/1000 | loss 11.8632\n",
      "Epoch 4: ================....  80.0% | loss: 6.40610E4 S800/1000 | loss 12.7539\n",
      "Epoch 4: ==================== 100.0% | loss: 4.59096\n",
      "E4 S1000/1000 | loss 10.4822\n",
      "[E4] checkpoint saved.\n",
      "Epoch 5: ====................  20.0% | loss: 27.7538E5 S200/1000 | loss 10.9001\n",
      "Epoch 5: ========............  40.0% | loss: 48.8085E5 S400/1000 | loss 11.3412\n",
      "Epoch 5: ============........  60.0% | loss: 4.08462E5 S600/1000 | loss 10.5248\n",
      "Epoch 5: ================....  80.0% | loss: 4.33404E5 S800/1000 | loss 9.8526\n",
      "Epoch 5: ==================== 100.0% | loss: 8.93232\n",
      "E5 S1000/1000 | loss 9.4419\n",
      "[E5] checkpoint saved.\n",
      "Epoch 6: ====................  20.0% | loss: 11.0864E6 S200/1000 | loss 11.3464\n",
      "Epoch 6: ========............  40.0% | loss: 8.36847E6 S400/1000 | loss 10.2836\n",
      "Epoch 6: ============........  60.0% | loss: 13.6542E6 S600/1000 | loss 10.6310\n",
      "Epoch 6: ================....  80.0% | loss: 7.86155E6 S800/1000 | loss 11.6776\n",
      "Epoch 6: ==================== 100.0% | loss: 4.07490\n",
      "E6 S1000/1000 | loss 10.6915\n",
      "[E6] checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "def train_single_stage(model, sampler, style_grams, num_epochs, stage_name=\"stage_single\"):\n",
    "    sampler.set_split('train')\n",
    "    scaler = torch.cuda.amp.GradScaler() if (USE_AMP and DEVICE.type == \"cuda\") else None\n",
    "\n",
    "    # define steps per epoch (bounded to data size but keep it reasonably large)\n",
    "    n_train = len(sampler.train_content_files)\n",
    "    steps_per_epoch = max(100, n_train // max(1, BATCH_SIZE))\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        ep_prefix = f\"[{stage_name}] Epoch {epoch}/{num_epochs}\"\n",
    "\n",
    "        for step in range(1, steps_per_epoch + 1):\n",
    "            content_batch, style_batch = sampler.sample_batch(BATCH_SIZE)\n",
    "            content_batch = content_batch.to(DEVICE)\n",
    "            # style_batch not needed directly since we use precomputed grams\n",
    "            opt.zero_grad()\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(content_batch)\n",
    "                    c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(output)\n",
    "                    c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm)\n",
    "\n",
    "                    # Content loss\n",
    "                    c_loss = sum(torch.mean((o_feats[l] - c_feats[l])**2) for l in content_layers)\n",
    "\n",
    "                    # Style loss\n",
    "                    Gs = random.choice(style_grams)\n",
    "                    s_loss = 0.0\n",
    "                    for l in style_layers:\n",
    "                        Go = gram_matrix_batch(o_feats[l])\n",
    "                        s_loss += style_weights.get(l, 1.0) * torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "\n",
    "                    # TV\n",
    "                    tv = tv_loss_fn(output)\n",
    "                    total_loss = CONTENT_WEIGHT * c_loss + STYLE_WEIGHT * s_loss + TV_WEIGHT * tv\n",
    "\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                output = model(content_batch)\n",
    "                c_norm, o_norm = normalize_for_vgg(content_batch), normalize_for_vgg(output)\n",
    "                c_feats, o_feats = vgg_feat(c_norm), vgg_feat(o_norm)\n",
    "\n",
    "                c_loss = sum(torch.mean((o_feats[l] - c_feats[l])**2) for l in content_layers)\n",
    "\n",
    "                Gs = random.choice(style_grams)\n",
    "                s_loss = 0.0\n",
    "                for l in style_layers:\n",
    "                    Go = gram_matrix_batch(o_feats[l])\n",
    "                    s_loss += style_weights.get(l, 1.0) * torch.mean((Go - Gs[l].to(DEVICE))**2)\n",
    "\n",
    "                tv = tv_loss_fn(output)\n",
    "                total_loss = CONTENT_WEIGHT * c_loss + STYLE_WEIGHT * s_loss + TV_WEIGHT * tv\n",
    "                total_loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "\n",
    "            # tiny '==> ....' progress\n",
    "            print_progress(f\"{ep_prefix}\", step, steps_per_epoch, every=10, end=False)\n",
    "\n",
    "            # occasional sample save\n",
    "            if step % 300 == 0 or step == steps_per_epoch:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    sample_out = model(content_batch[:1]).cpu()\n",
    "                    utils.save_image(sample_out, f\"{SAMPLES_DIR}/{stage_name}_ep{epoch}_step{step}.png\")\n",
    "                model.train()\n",
    "\n",
    "        avg_loss = epoch_loss / steps_per_epoch\n",
    "        utils.save_image(output[:1].detach().cpu(), f\"{SAMPLES_DIR}/{stage_name}_ep{epoch}_lastbatch.png\")\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"{stage_name}_epoch{epoch}.pth\"))\n",
    "        print_progress(f\"{ep_prefix} | Avg loss {avg_loss:.4f}\", steps_per_epoch, steps_per_epoch, end=True)\n",
    "        print(f\"{ep_prefix} | Avg loss {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting single-stage training...\")\n",
    "train_single_stage(model, sampler, style_grams, NUM_EPOCHS, stage_name=f\"stage_single_{IMG_SIZE}\")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stylized image to ./samples_nststyle/stylized_image.png\n"
     ]
    }
   ],
   "source": [
    "ckpt = os.path.join(CHECKPOINT_DIR, f\"stage_single_{IMG_SIZE}_epoch{NUM_EPOCHS}.pth\")\n",
    "if os.path.exists(ckpt):\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=DEVICE))\n",
    "    print(\"Loaded checkpoint:\", ckpt)\n",
    "else:\n",
    "    print(\"Checkpoint not found; using in-memory weights.\")\n",
    "\n",
    "model.eval()\n",
    "test_img_path = \"test_imgs/butterfly.jpg\"\n",
    "if not os.path.exists(test_img_path):\n",
    "    print(\"Test image not found at\", test_img_path)\n",
    "else:\n",
    "    img = exif_fix_and_open(test_img_path)\n",
    "    tf  = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                              transforms.CenterCrop(IMG_SIZE),\n",
    "                              transforms.ToTensor()])\n",
    "    content_tensor = tf(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(content_tensor)\n",
    "    out_path = os.path.join(SAMPLES_DIR, \"stylized_image.png\")\n",
    "    utils.save_image(out.cpu(), out_path)\n",
    "    print(\"Saved stylized image to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
